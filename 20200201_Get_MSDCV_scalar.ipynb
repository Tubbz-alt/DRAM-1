{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get FF_estimation_scalar MSDCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that analysis_estimation_nds.py imports FF_estimation_scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import analysis_estimation_nds\n",
    "import importlib\n",
    "importlib.reload(analysis_estimation_nds)\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style()\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose which model is of interest, specify the test condition and if the data is created incrementally, specify the results directory, and set scalar to True and specify if the results should be rounded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [(\"New_DAA_decrs_fN_scalar\", \"$f(n) \\propto 1/n^2$, Decr Avg Area\"),\n",
    "          (\"New_DAA_const_fN_scalar\", \"$f(n) \\propto 1/n^2$, Const Avg Area\"),\n",
    "          (\"New_CAA_decrs_fN_scalar\", \"$f(n)$ Constant, Decr Avg Area\"),\n",
    "          (\"New_CAA_const_fN_scalar\", \"$f(n)$ Constant, Const Avg Area\")]\n",
    "model = models[3]\n",
    "model_name, model_figure_title = model\n",
    "num_runs = 10\n",
    "iter_list = [0, 200, 400, 1600, 6400, 25600, 102400, 204800, 409600, 819200, 1000000, 1228800, 1638400, 2000000, 3000000]#, 4000000, 5000000, 6000000]\n",
    "max_blobs = 9\n",
    "min_blobs = 1\n",
    "\n",
    "num_iters = len(iter_list)\n",
    "output_size = max_blobs - min_blobs + 1\n",
    "test_condition = \"po_inc\"\n",
    "incremental = True\n",
    "data_directory = \"data/\" + model_name + \"/\" + test_condition + \"/\"\n",
    "\n",
    "import os\n",
    "if not os.path.exists(data_directory):\n",
    "    os.makedirs(data_directory)\n",
    "    \n",
    "from pathlib import Path\n",
    "results_dir = 'Results/20200307_msdcv_3000000_scalar/unrounded/' + test_condition + \"/\"\n",
    "Path(results_dir).mkdir(parents=True, exist_ok=True)\n",
    "scalar = True\n",
    "rounded = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n",
    "import os\n",
    "path = 'model_runs/' + model_name + str(1)\n",
    "it = 0\n",
    "checkpoint_path = \"%s/classifymodel_%d.ckpt\" % (path, it)\n",
    "\n",
    "# List ALL tensors example output: v0/Adam (DT_FLOAT) [3,3,1,80]\n",
    "print_tensors_in_checkpoint_file(file_name=checkpoint_path, tensor_name='', all_tensors=False, all_tensor_names=False)\n",
    "\n",
    "# List contents of v0 tensor.\n",
    "# Example output: tensor_name:  v0 [[[[  9.27958265e-02   7.40226209e-02   4.52989563e-02   3.15700471e-02\n",
    "# print_tensors_in_checkpoint_file(file_name=checkpoint_path, tensor_name='hidden/b', all_tensors=False, all_tensor_names=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path = 'model_runs/' + model_name + str(1)\n",
    "imgs_data = analysis_estimation_nds.classify_imgs_po(0, True, 100, path=path, incremental=incremental, scalar=scalar)\n",
    "print(np.shape(imgs_data[1][\"classifications\"]))\n",
    "print(imgs_data[1][\"classifications\"][0])\n",
    "print(imgs_data[1][\"classifications\"][0][0]) # scalar output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifications_one_run(path, iteration):\n",
    "    \"\"\"Get classifications for one run at one iteration.\"\"\"\n",
    "\n",
    "    imgs_data = analysis_estimation_nds.classify_imgs_po(iteration, True, 100, path=path, incremental=incremental, scalar=scalar) # new_imgs = True\n",
    "    unrounded_values = {nb: [] for nb in range(1, output_size+1)}\n",
    "    rounded_values = {nb: [] for nb in range(1, output_size+1)}\n",
    "    \n",
    "    for idx, data in enumerate(imgs_data):\n",
    "        nb = data[\"label\"][0]\n",
    "        classification = data[\"classifications\"][0][0]\n",
    "        unrounded_values[nb].append(classification)\n",
    "        rounded_values[nb].append(round(classification))\n",
    "    return unrounded_values, rounded_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unrounded_values_all_runs = []\n",
    "rounded_values_all_runs = []\n",
    "\n",
    "for run in range(num_runs):\n",
    "    path = 'model_runs/' + model_name + str(run + 1) # '/run_' if all the runs are stored in one folder\n",
    "    unrounded_values_all_iters = []\n",
    "    rounded_values_all_iters = []\n",
    "    for i, iteration in enumerate(iter_list):\n",
    "        uvals, rvals = get_classifications_one_run(path, iteration)\n",
    "        unrounded_values_all_iters.append(uvals)\n",
    "        rounded_values_all_iters.append(rvals)\n",
    "    unrounded_values_all_runs.append(unrounded_values_all_iters)\n",
    "    rounded_values_all_runs.append(rounded_values_all_iters)\n",
    "    \n",
    "with open(data_directory + 'unrounded_values_all_runs', 'wb') as fp:\n",
    "    pickle.dump(unrounded_values_all_runs, fp)\n",
    "    \n",
    "with open(data_directory + 'rounded_values_all_runs', 'wb') as fp:\n",
    "    pickle.dump(rounded_values_all_runs, fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_msdcv(classifications_list):\n",
    "    \"\"\"Get the mean, standard deviation, and coefficient of variation matrices from classifications list.\"\"\"\n",
    "    \n",
    "    msdcv = np.zeros([num_iters, output_size, 3])    \n",
    "    values = [{nb: [] for nb in range(1, output_size+1)} for i in range(num_iters)] # iter x nb x value\n",
    "    for run in range(num_runs):\n",
    "        for i in range(num_iters):\n",
    "            for t in range(output_size):\n",
    "                nb = t + 1\n",
    "                if nb in classifications_list[run][i]:\n",
    "                    values[i][nb] += classifications_list[run][i][nb]\n",
    "    for i in range(num_iters):\n",
    "        for t in range(output_size):\n",
    "            nb = t + 1\n",
    "            vals = values[i][nb]\n",
    "            msdcv[i, t, 0] = mu = np.mean(vals)\n",
    "            msdcv[i, t, 1] = sigma = np.std(vals)\n",
    "            msdcv[i, t, 2] = cv = sigma / mu\n",
    "    return msdcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrounded_msdcv = get_msdcv(unrounded_values_all_runs)\n",
    "rounded_msdcv = get_msdcv(rounded_values_all_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(data_directory + \"unrounded_msdcv\", unrounded_msdcv)\n",
    "np.save(data_directory + \"rounded_msdcv\", rounded_msdcv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Mean, Standard Deviation, and Coefficient of Variation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 20200202_Plot_MSDCV_scalar.ipynb to get plots. Below is an example plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_iters = [0, 200, 400, 1600, 6400, 6400, 102400, 204800, 409600, 819200, 1000000, 1228800, 1638400, 2000000, 3000000]#, 4000000, 5000000, 6000000]\n",
    "iters = [3000000]\n",
    "iter_indices = [i for i, iteration in enumerate(all_iters) if iteration in iters]\n",
    "max_blobs = 9\n",
    "min_blobs = 1\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(3.7*3, 3.6))\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "plt.xticks(np.arange(min_blobs, max_blobs+1, step=1))\n",
    "ax[0].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax[1].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax[2].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "numerosity = np.arange(min_blobs, max_blobs+1)\n",
    "\n",
    "ax[0].set_title(\"Mean\")\n",
    "ax[0].set_xlabel(\"Stimulus Numerosity\")\n",
    "ax[0].set_ylabel(\"Numerosity Estimate\")\n",
    "ax[0].set_yticks(np.arange(min_blobs, max_blobs+1, step=1))\n",
    "ax[0].set_xticks(np.arange(min_blobs, max_blobs+1, step=1))\n",
    "ax[0].set_prop_cycle('color',plt.cm.Spectral([val for val in np.linspace(0,1,len(iter_indices)) for _ in (0,1)]))\n",
    "\n",
    "ax[1].set_title(\"Standard Deviation\")\n",
    "ax[1].set_xlabel(\"Stimulus Numerosity\")\n",
    "ax[1].set_xticks(np.arange(min_blobs, max_blobs+1, step=1))\n",
    "ax[1].set_prop_cycle('color',plt.cm.Spectral([val for val in np.linspace(0,1,len(iter_indices)) for _ in (0,1)]))\n",
    "\n",
    "ax[2].set_title(\"Coefficient of Variation\")\n",
    "ax[2].set_xlabel(\"Stimulus Numerosity\")\n",
    "# ax[2].set_yticks(np.arange(0, 1, step=0.02))\n",
    "ax[2].set_xticks(np.arange(min_blobs, max_blobs+1, step=1))\n",
    "ax[2].set_prop_cycle('color',plt.cm.Spectral([val for val in np.linspace(0,1,len(iter_indices)) for _ in (0,1)]))\n",
    "\n",
    "for i in iter_indices:\n",
    "    mean = unrounded_msdcv[i,min_blobs-1:max_blobs,0]\n",
    "    sd = unrounded_msdcv[i,min_blobs-1:max_blobs+1,1]\n",
    "    cv = unrounded_msdcv[i,min_blobs-1:max_blobs+1,2]\n",
    "\n",
    "    # Plot Mean.\n",
    "    ax[0].plot(numerosity, mean, label='N')\n",
    "    \n",
    "    # Plot SD.\n",
    "    ax[1].plot(numerosity, sd, label='N')\n",
    "    \n",
    "    # Plot CV.\n",
    "    ax[2].plot(numerosity, cv, label='N')\n",
    "\n",
    "# legend = ax[i, 1].legend()\n",
    "fig.tight_layout()\n",
    "fig.suptitle(\"Final Performance: %s\" % model_figure_title)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "from pathlib import Path\n",
    "plt.savefig(results_dir + model_name + '.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrounded_msdcv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
